{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43cd0283",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67bc25b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bene/projects/generative-models/examples/torch_autoencoder'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f49cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bene/projects/generative-models\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bene/projects/generative-models'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd ../..\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8b37257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9313671a3e57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgempy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgempy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgempy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_encoder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoEncoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gempy.torch.encoder import ConvEncoder\n",
    "from gempy.torch.decoder import ConvDecoder\n",
    "from gempy.torch.auto_encoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dfd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = (1, 28, 28)\n",
    "z_dim = (4, )\n",
    "z_labels = ('z', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a595281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    print (f'GPU available')\n",
    "    device = 'cuda'\n",
    "\n",
    "x_random = torch.randn(1, *input_dim)\n",
    "x_random = x_random.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d16f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvEncoder(\n",
    "    input_dim=input_dim,\n",
    "    filters=(32, 64, 64, 64),\n",
    "    #filters=(8, 16, 16, 16),\n",
    "    kernel_size=(3, 3, 3, 3),\n",
    "    strides=(1, 2, 2, 1),\n",
    "    activation='leaky_relu',\n",
    "    latent_dim=z_dim,\n",
    "    latent_labels=z_labels,\n",
    "    latent_activation='sigmoid',\n",
    ")\n",
    "\n",
    "encoer = encoder.to(device)\n",
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136095e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = ConvDecoder(\n",
    "    latent_dim=z_dim,\n",
    "    latent_upscale=(64, 3, 3),\n",
    "    filters=[64, 64, 32, 1],\n",
    "    #filters=[16, 16, 8, 1],\n",
    "    kernel_size=[3, 4, 4, 3],\n",
    "    strides=[1, 2, 2, 1],\n",
    "    activation=['leaky_relu', 'leaky_relu', 'leaky_relu', 'sigmoid'],\n",
    "    latent_merge=False,\n",
    "    latent_activation=None,\n",
    ")\n",
    "\n",
    "decoder = decoder.to('cuda')\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c37430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for l in decoder.conv_stack:\n",
    "    print('layer activation: ', l[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93979f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_encoder = AutoEncoder(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    ")\n",
    "\n",
    "auto_encoder = auto_encoder.to(device)\n",
    "auto_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79148b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('input shape     :', auto_encoder.encoder.conv_stack_shape_in)\n",
    "print('latent shape     :', auto_encoder.encoder.conv_stack_shape_out)\n",
    "print('z shape          :', z_dim)\n",
    "print('output shape    :', auto_encoder.decoder.conv_stack_shape_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = auto_encoder(x_random)\n",
    "\n",
    "print('latent space    :', encoder.latent_torch)\n",
    "print('output shape    :', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c57f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in auto_encoder.parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bc077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_cifar10(batch_size, dataset_directory, dataloader_workers):\n",
    "    # Prepare dataset for training\n",
    "    train_transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    ])\n",
    "    train_dataset = torchvision.datasets.CIFAR10(root=dataset_directory, train=True,\n",
    "                                                 download=True, transform=train_transformation)\n",
    "    \n",
    "    test_dataset =  torchvision.datasets.CIFAR10(root=dataset_directory, train=False,\n",
    "                                                 download=True, transform=train_transformation)\n",
    "\n",
    "    # Prepare Data Loaders for training and validation\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               pin_memory=True, num_workers=dataloader_workers)\n",
    "\n",
    "    return train_dataset, train_loader\n",
    "\n",
    "def get_MNIST(batch_size, dataset_directory, dataloader_workers):\n",
    "    # Prepare dataset for training\n",
    "    train_transformation = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(root=dataset_directory, train=True,\n",
    "                                               download=True, transform=train_transformation)\n",
    "    \n",
    "    test_dataset =  torchvision.datasets.MNIST(root=dataset_directory, train=False,\n",
    "                                               download=True, transform=train_transformation)\n",
    "\n",
    "    # Prepare Data Loaders for training and validation\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                                               pin_memory=True, num_workers=dataloader_workers)\n",
    "\n",
    "    return train_dataset, train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286c50b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 5e-4\n",
    "\n",
    "train_dataset, train_loader = get_MNIST(batch_size, 'examples/torch_autoencoder/data/MNIST', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an optimizer object\n",
    "optimizer = torch.optim.Adam(auto_encoder.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51b3148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean-squared error loss\n",
    "criterion = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb58df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(train_dataset[10][0].detach().numpy().transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e078b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for batch_features, _ in train_loader:\n",
    "        # reshape mini-batch data to [N, 784] matrix\n",
    "        # load it to the active device\n",
    "        # batch_features = batch_features.view(-1, 784).to(device)\n",
    "        batch_features = batch_features.to(device)\n",
    "        \n",
    "        # reset the gradients back to zero\n",
    "        # PyTorch accumulates gradients on subsequent backward passes\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute reconstructions\n",
    "        outputs = auto_encoder(batch_features)\n",
    "        \n",
    "        # compute training reconstruction loss\n",
    "        train_loss = criterion(outputs, batch_features)\n",
    "        \n",
    "        # compute accumulated gradients\n",
    "        train_loss.backward()\n",
    "        \n",
    "        # perform parameter update based on current gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # add the mini-batch training loss to epoch loss\n",
    "        loss += train_loss.item()\n",
    "    \n",
    "    # compute the epoch training loss\n",
    "    loss = loss / len(train_loader)\n",
    "    \n",
    "    # display the epoch training loss\n",
    "    print(\"epoch : {}/{}, loss = {:.6f}\".format(epoch + 1, epochs, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "z_rand = np.random.rand(*z_dim)\n",
    "t_rand = torch.from_numpy(z_rand[None, ...]).float().to(device)\n",
    "\n",
    "y_rand = auto_encoder.decoder(t_rand)\n",
    "\n",
    "y = y_rand[0].detach().cpu().numpy()\n",
    "plt.imshow(y.transpose(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3fc15b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gempy",
   "language": "python",
   "name": "gempy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
